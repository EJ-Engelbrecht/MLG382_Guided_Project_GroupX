{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:54: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:54: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Karl\\AppData\\Local\\Temp\\ipykernel_25560\\2148462400.py:54: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df = load_data('..\\data\\Student_performance_data.csv')\n",
      "c:\\Users\\Karl\\source\\repos\\ApexCareSolutions\\MLG382_Guided_Project_GroupX\\src\\preprocess_data.py:53: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace({False: 0.0, True: 1.0})\n",
      "C:\\Users\\Karl\\AppData\\Local\\Temp\\ipykernel_25560\\2148462400.py:54: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df = load_data('..\\data\\Student_performance_data.csv')\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[17:53:10] C:\\b\\abs_90_bwj_86a\\croot\\xgboost-split_1724073762025\\work\\src\\objective\\multiclass_obj.cu:111: SoftmaxMultiClassObj: label must be in [0, num_class).",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mXGBoostError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m     83\u001b[39m num_round = \u001b[32m100\u001b[39m  \u001b[38;5;66;03m# Number of boosting rounds\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m bst = xgb.train(params, dtrain, num_round)\n\u001b[32m     86\u001b[39m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[32m     87\u001b[39m y_pred = bst.predict(dtest)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\xgboost\\core.py:726\u001b[39m, in \u001b[36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    724\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig.parameters, args):\n\u001b[32m    725\u001b[39m     kwargs[k] = arg\n\u001b[32m--> \u001b[39m\u001b[32m726\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m func(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\xgboost\\training.py:181\u001b[39m, in \u001b[36mtrain\u001b[39m\u001b[34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.before_iteration(bst, i, dtrain, evals):\n\u001b[32m    180\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m181\u001b[39m bst.update(dtrain, iteration=i, fobj=obj)\n\u001b[32m    182\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cb_container.after_iteration(bst, i, dtrain, evals):\n\u001b[32m    183\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\xgboost\\core.py:2100\u001b[39m, in \u001b[36mBooster.update\u001b[39m\u001b[34m(self, dtrain, iteration, fobj)\u001b[39m\n\u001b[32m   2097\u001b[39m \u001b[38;5;28mself\u001b[39m._assign_dmatrix_features(dtrain)\n\u001b[32m   2099\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2100\u001b[39m     _check_call(\n\u001b[32m   2101\u001b[39m         _LIB.XGBoosterUpdateOneIter(\n\u001b[32m   2102\u001b[39m             \u001b[38;5;28mself\u001b[39m.handle, ctypes.c_int(iteration), dtrain.handle\n\u001b[32m   2103\u001b[39m         )\n\u001b[32m   2104\u001b[39m     )\n\u001b[32m   2105\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2106\u001b[39m     pred = \u001b[38;5;28mself\u001b[39m.predict(dtrain, output_margin=\u001b[38;5;28;01mTrue\u001b[39;00m, training=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\xgboost\\core.py:284\u001b[39m, in \u001b[36m_check_call\u001b[39m\u001b[34m(ret)\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Check the return value of C API call\u001b[39;00m\n\u001b[32m    274\u001b[39m \n\u001b[32m    275\u001b[39m \u001b[33;03mThis function will raise exception when error occurs.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    281\u001b[39m \u001b[33;03m    return value from API calls\u001b[39;00m\n\u001b[32m    282\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ret != \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m XGBoostError(py_str(_LIB.XGBGetLastError()))\n",
      "\u001b[31mXGBoostError\u001b[39m: [17:53:10] C:\\b\\abs_90_bwj_86a\\croot\\xgboost-split_1724073762025\\work\\src\\objective\\multiclass_obj.cu:111: SoftmaxMultiClassObj: label must be in [0, num_class)."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt  \n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load dataset)\n",
    "#Importing Required Libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import the python libraies from source\n",
    "#Allows for model to reload without reloading Kernel\n",
    "import importlib \n",
    "\n",
    "#Python files can be used \n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "#prepare_data.py file importing functions\n",
    "from prepare_data import (\n",
    "    load_data,\n",
    "    catagorical_column_transformations,\n",
    "    feature_engineering\n",
    "    \n",
    ")\n",
    "#preprocess_data.py file importing functions\n",
    "from preprocess_data import (\n",
    "    scale_and_encode,\n",
    "    remove_anomalies,\n",
    "    get_numeric_columns,\n",
    "    iterative_outlier_removal,\n",
    "    make_Onehot\n",
    ")\n",
    "#train_models.py file importing functions\n",
    "from train_models import (\n",
    "    split_features_target,\n",
    "    create_train_test_split,\n",
    "    get_model,\n",
    "    train_model\n",
    ")\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "# load the data using the load_data function from prepare_data.py\n",
    "df = load_data('..\\data\\Student_performance_data.csv')\n",
    "# decode the catagorical features\n",
    "df = catagorical_column_transformations(df)\n",
    "# perform feature engineering using the feature_engineering function from prepare_data.py\n",
    "df = feature_engineering(df)\n",
    "numeric_columns = get_numeric_columns(df)\n",
    "df = scale_and_encode(df)\n",
    "# iterative outlier removal using the iterative_outlier_removal function from preprocess_data.py\n",
    "df = iterative_outlier_removal(df, numeric_columns)\n",
    "df = make_Onehot(df)\n",
    "X, y = split_features_target(df)\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create an XGBoost DMatrix\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set the parameters for the XGBoost model\n",
    "params = {\n",
    "    'objective': 'multi:softmax',  # Specify multiclass classification\n",
    "    'num_class': 1,                 # Number of classes in the target variable\n",
    "    'max_depth': 3,                 # Maximum depth of a tree\n",
    "    'eta': 0.1,                     # Learning rate\n",
    "    'eval_metric': 'mlogloss'       # Evaluation metric\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "num_round = 100  # Number of boosting rounds\n",
    "bst = xgb.train(params, dtrain, num_round)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = bst.predict(dtest)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.2f}')\n",
    "\n",
    "#Grade mapping\n",
    "grade_map = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'F'}\n",
    "y_test_letters = pd.Series(y_test).map(grade_map)\n",
    "y_pred_letters = pd.Series(y_pred).map(grade_map)\n",
    "label_names = [grade_map[i] for i in sorted(grade_map)]\n",
    "\n",
    "#Step 8: Evaluation\n",
    "accuracy = accuracy_score(y_test_letters, y_pred_letters)\n",
    "print(\"XGBoost Accuracy:\", round(accuracy, 4))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_letters, y_pred_letters, target_names=label_names, zero_division=0))\n",
    "\n",
    "#Step 11: Save the model as pkl file in artifacts\n",
    "\n",
    "with open(\"../artifacts/xgboost_model.pkl\", \"wb\") as file:\n",
    "   pickle.dump(bst, file)\n",
    "\n",
    "#Step 12: Save predictions to CSV\n",
    "comp_df = X_test.copy()\n",
    "comp_df[\"Actual_GradeClass\"] = y_test.values\n",
    "comp_df[\"Predicted_GradeClass\"] = y_pred\n",
    "\n",
    "#Step 13: Show and save the predictions table\n",
    "try:\n",
    "    from IPython.display import display\n",
    "\n",
    "    # Prepare DataFrame for display\n",
    "    comp_df = pd.DataFrame({\"Actual\": y_test.values,\"Predicted\": y_pred})\n",
    "    comp_df[\"Match\"] = comp_df[\"Actual\"] == comp_df[\"Predicted\"]\n",
    "\n",
    "    def highlight_false_text(row):\n",
    "        styles = []\n",
    "        for col in row.index:\n",
    "            if col == \"Match\" and row[\"Match\"] == False:\n",
    "                styles.append(\"color: red; background-color: black\")\n",
    "            else:\n",
    "                styles.append(\"background-color: black; color: white\")\n",
    "        return styles\n",
    "\n",
    "    print(\"First 20 Predictions:\")\n",
    "    display(comp_df.head(20).style.apply(highlight_false_text, axis=1))\n",
    "\n",
    "except Exception as e:\n",
    "    print(\"\\n First 20 Predictions:\")\n",
    "    print(comp_df.head(20).to_string(index=False))\n",
    "\n",
    "comp_df.to_csv(\"../artifacts/xgboost_prediction.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:38: SyntaxWarning: invalid escape sequence '\\d'\n",
      "<>:38: SyntaxWarning: invalid escape sequence '\\d'\n",
      "C:\\Users\\Karl\\AppData\\Local\\Temp\\ipykernel_13752\\2333588591.py:38: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df = load_data('..\\data\\Student_performance_data.csv')\n",
      "c:\\Users\\Karl\\source\\repos\\ApexCareSolutions\\MLG382_Guided_Project_GroupX\\src\\preprocess_data.py:53: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df[col] = df[col].replace({False: 0.0, True: 1.0})\n",
      "C:\\Users\\Karl\\AppData\\Local\\Temp\\ipykernel_13752\\2333588591.py:38: SyntaxWarning: invalid escape sequence '\\d'\n",
      "  df = load_data('..\\data\\Student_performance_data.csv')\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'super' object has no attribute '__sklearn_tags__'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 63\u001b[39m\n\u001b[32m     61\u001b[39m model=get_model(\u001b[33m'\u001b[39m\u001b[33mxgboost\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     62\u001b[39m \u001b[38;5;66;03m# model training and evaliation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m train_model(model, \u001b[33m'\u001b[39m\u001b[33mxgboost\u001b[39m\u001b[33m'\u001b[39m, X_train, X_test, Y_train, Y_test)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\source\\repos\\ApexCareSolutions\\MLG382_Guided_Project_GroupX\\src\\train_models.py:136\u001b[39m, in \u001b[36mtrain_model\u001b[39m\u001b[34m(model, model_name, X_train, X_test, Y_train, Y_test, output_dir)\u001b[39m\n\u001b[32m    133\u001b[39m pipeline.fit(X_train, Y_train)\n\u001b[32m    135\u001b[39m \u001b[38;5;66;03m#Predicts test set results\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m Y_pred = pipeline.predict(X_test)\n\u001b[32m    138\u001b[39m \u001b[38;5;66;03m#Prints evaluatoin metrics for each model\u001b[39;00m\n\u001b[32m    139\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\sklearn\\pipeline.py:782\u001b[39m, in \u001b[36mPipeline.predict\u001b[39m\u001b[34m(self, X, **params)\u001b[39m\n\u001b[32m    740\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Transform the data, and apply `predict` with the final estimator.\u001b[39;00m\n\u001b[32m    741\u001b[39m \n\u001b[32m    742\u001b[39m \u001b[33;03mCall `transform` of each transformer in the pipeline. The transformed\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    779\u001b[39m \u001b[33;03m    Result of calling `predict` on the final estimator.\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    781\u001b[39m \u001b[38;5;66;03m# TODO(1.8): Remove the context manager and use check_is_fitted(self)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m782\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m _raise_or_warn_if_not_fitted(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    783\u001b[39m     Xt = X\n\u001b[32m    785\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _routing_enabled():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\contextlib.py:144\u001b[39m, in \u001b[36m_GeneratorContextManager.__exit__\u001b[39m\u001b[34m(self, typ, value, traceback)\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m         \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m.gen)\n\u001b[32m    145\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[32m    146\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\sklearn\\pipeline.py:60\u001b[39m, in \u001b[36m_raise_or_warn_if_not_fitted\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;66;03m# we only get here if the above didn't raise\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     check_is_fitted(estimator)\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError:\n\u001b[32m     62\u001b[39m     warnings.warn(\n\u001b[32m     63\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThis Pipeline instance is not fitted yet. Call \u001b[39m\u001b[33m'\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m'\u001b[39m\u001b[33m with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mappropriate arguments before using other methods such as transform, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m     67\u001b[39m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[32m     68\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\sklearn\\utils\\validation.py:1756\u001b[39m, in \u001b[36mcheck_is_fitted\u001b[39m\u001b[34m(estimator, attributes, msg, all_or_any)\u001b[39m\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tags.requires_fit \u001b[38;5;129;01mand\u001b[39;00m attributes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _is_fitted(estimator, attributes, all_or_any):\n\u001b[32m   1757\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg % {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator).\u001b[34m__name__\u001b[39m})\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\sklearn\\utils\\validation.py:1665\u001b[39m, in \u001b[36m_is_fitted\u001b[39m\u001b[34m(estimator, attributes, all_or_any)\u001b[39m\n\u001b[32m   1662\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_or_any([\u001b[38;5;28mhasattr\u001b[39m(estimator, attr) \u001b[38;5;28;01mfor\u001b[39;00m attr \u001b[38;5;129;01min\u001b[39;00m attributes])\n\u001b[32m   1664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33m__sklearn_is_fitted__\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1665\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m estimator.__sklearn_is_fitted__()\n\u001b[32m   1667\u001b[39m fitted_attrs = [\n\u001b[32m   1668\u001b[39m     v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v.endswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v.startswith(\u001b[33m\"\u001b[39m\u001b[33m__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1669\u001b[39m ]\n\u001b[32m   1670\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(fitted_attrs) > \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\sklearn\\pipeline.py:1321\u001b[39m, in \u001b[36mPipeline.__sklearn_is_fitted__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1314\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1316\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1317\u001b[39m     \u001b[38;5;66;03m# check if the last step of the pipeline is fitted\u001b[39;00m\n\u001b[32m   1318\u001b[39m     \u001b[38;5;66;03m# we only check the last step since if the last step is fit, it\u001b[39;00m\n\u001b[32m   1319\u001b[39m     \u001b[38;5;66;03m# means the previous steps should also be fit. This is faster than\u001b[39;00m\n\u001b[32m   1320\u001b[39m     \u001b[38;5;66;03m# checking if every step of the pipeline is fit.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1321\u001b[39m     check_is_fitted(last_step)\n\u001b[32m   1322\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   1323\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m NotFittedError:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\sklearn\\utils\\validation.py:1751\u001b[39m, in \u001b[36mcheck_is_fitted\u001b[39m\u001b[34m(estimator, attributes, msg, all_or_any)\u001b[39m\n\u001b[32m   1748\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(estimator, \u001b[33m\"\u001b[39m\u001b[33mfit\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m is not an estimator instance.\u001b[39m\u001b[33m\"\u001b[39m % (estimator))\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m tags = get_tags(estimator)\n\u001b[32m   1753\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tags.requires_fit \u001b[38;5;129;01mand\u001b[39;00m attributes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1754\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\sklearn\\utils\\_tags.py:430\u001b[39m, in \u001b[36mget_tags\u001b[39m\u001b[34m(estimator)\u001b[39m\n\u001b[32m    428\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m klass \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mtype\u001b[39m(estimator).mro()):\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m__sklearn_tags__\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n\u001b[32m--> \u001b[39m\u001b[32m430\u001b[39m         sklearn_tags_provider[klass] = klass.__sklearn_tags__(estimator)  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    431\u001b[39m         class_order.append(klass)\n\u001b[32m    432\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m_more_tags\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(klass):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Karl\\anaconda3\\envs\\MLG382_class\\Lib\\site-packages\\sklearn\\base.py:613\u001b[39m, in \u001b[36mRegressorMixin.__sklearn_tags__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__sklearn_tags__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     tags = \u001b[38;5;28msuper\u001b[39m().__sklearn_tags__()\n\u001b[32m    614\u001b[39m     tags.estimator_type = \u001b[33m\"\u001b[39m\u001b[33mregressor\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    615\u001b[39m     tags.regressor_tags = RegressorTags()\n",
      "\u001b[31mAttributeError\u001b[39m: 'super' object has no attribute '__sklearn_tags__'"
     ]
    }
   ],
   "source": [
    "#Importing Required Libraries\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import the python libraies from source\n",
    "#Allows for model to reload without reloading Kernel\n",
    "import importlib \n",
    "\n",
    "#Python files can be used \n",
    "sys.path.append(os.path.abspath(\"../src\"))\n",
    "\n",
    "#prepare_data.py file importing functions\n",
    "from prepare_data import (\n",
    "    load_data,\n",
    "    catagorical_column_transformations,\n",
    "    feature_engineering\n",
    "    \n",
    ")\n",
    "#preprocess_data.py file importing functions\n",
    "from preprocess_data import (\n",
    "    scale_and_encode,\n",
    "    remove_anomalies,\n",
    "    get_numeric_columns,\n",
    "    iterative_outlier_removal,\n",
    "    make_Onehot\n",
    ")\n",
    "#train_models.py file importing functions\n",
    "from train_models import (\n",
    "    split_features_target,\n",
    "    create_train_test_split,\n",
    "    get_model,\n",
    "    train_model\n",
    ")\n",
    "\n",
    "\n",
    "# Data preparation\n",
    "# load the data using the load_data function from prepare_data.py\n",
    "df = load_data('..\\data\\Student_performance_data.csv')\n",
    "# decode the catagorical features\n",
    "df = catagorical_column_transformations(df)\n",
    "# perform feature engineering using the feature_engineering function from prepare_data.py\n",
    "df = feature_engineering(df)\n",
    "# Preprocessing\n",
    "# scale and encode the data using the scale_and_encode function from preprocess_data.py\n",
    "df = scale_and_encode(df)\n",
    "#print(df.columns.to_list())\n",
    "# remove anomalies using the remove_anomalies function from preprocess_data.py\n",
    "#   df = remove_anomalies(df) ~ Removing the anaomalies might break the model\n",
    "\n",
    "# get the numeric columns using the get_numeric_columns function from preprocess_data.py\n",
    "numeric_columns = get_numeric_columns(df)\n",
    "# iterative outlier removal using the iterative_outlier_removal function from preprocess_data.py\n",
    "df = iterative_outlier_removal(df, numeric_columns)\n",
    "# make the data one-hot\n",
    "df = make_Onehot(df)\n",
    "# Model preparation\n",
    "X, y = split_features_target(df)\n",
    "# create an train test plit\n",
    "X_train, X_test, Y_train, Y_test = create_train_test_split(X, y)\n",
    "# reloading and training the deeplearning model\n",
    "model=get_model('xgboost')\n",
    "# model training and evaliation\n",
    "train_model(model, 'xgboost', X_train, X_test, Y_train, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLG382_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
